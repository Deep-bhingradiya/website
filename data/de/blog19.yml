- day: 30
  month: Jan
  title: "RedHat OpenShift 4.2 – Installations Pitfalls, Networking und Storage"
  subtitle: |
            <img src="/images/blogposts/OpenShift 4.2.png" alt="OpenShift 4.2"/>
            <a href="https://www.puzzle.ch/de/home">Puzzle ITC</a>, einer der Gründerfirmen von APPUiO, hat eine Woche genutzt, um ihr Wissen rund um 4.2 zu vertiefen. In der sogenannten  <a href="https://www.puzzle.ch/de/blog/articles/2020/01/15/mid-week">/mid Week</a> erzielten die Members spannende Erkenntnisse bei der Installation, Networking und Storage. r </a>.
  details: |
             <p>Der Blog ist aufgeteilt in drei Themen:</p>
             <li><b>Installations-Pitfalls:</b> In der /mid Week haben wir OpenShift 4.2 sowohl auf einem VMware vSphere Cluster wie auch bei AWS, GCP und Azure installiert. Wir berichten über die Probleme, die dabei aufgetaucht sind.</li>
             <li><b>Networking:</b> Je nach Cloud Plattform gibt es logischerweise Unterschiede (z.B. Load Balancer). Wir haben aber auch Unterschiede zwischen OpenShift 3.11 und 4.2 festgestellt.</li>
             <li><b>Storage:</b> Jeder Cloud Provider bietet diverse Typen von Storage zur Integration in OpenShift an. Wir zeigen euch, was es für Möglichkeiten gibt.</li>
             <h4>Installation Pitfalls</h4>
             <img src="/images/blogposts/Pitfalls.jpg" alt="Pitfalls"/>
             <p>Wenn du in diesem Blog nach einer Anleitung suchst, wie OpenShift 4.2 installiert wird, bist du leider falsch. Für die Installationen haben auch wir die offiziellen <a href="https://docs.openshift.com/container-platform/4.2/welcome/index.html">Anleitungen von RedHat</a> verwendet.</p>
             <p>Vorneweg: Die Installation wird mit OpenShift Version 4.2 deutlich einfacher und schneller als mit 3.11. Generell - egal für welche Zielplattform - haben wir Folgendes festgestellt:</p>
             <li>Kleinere Umgebungen als 3 Master / 3 Node (Standardvorgabe) sollten nicht gewählt werden. Es wird sehr langsam oder die Installation schlägt teilweise sogar fehl. </li>
             <li>Für den OpenShift Installer wird ein <em>install-config.yaml</em> File erstellt. Dies sollte vor Beginn der Installation gesichert werden, da der Installer dieses anschliessend löscht.</li>
             <li>Weiter sollten auch alle Terraform Output Files gesichert werden, damit später der Cluster einfach gelöscht werden kann. Achtung, beim Löschen des Cluster erfolgt keine zusätzliche Bestätigung!</li>
             <li>Je nach Plattform haben wir unterschiedliche Grössen der Standard-VM festgestellt.</li>
             <li>Wenn <a href="https://docs.openshift.com/container-platform/4.2/installing/installing_aws/installing-aws-customizations.html#ssh-agent-using_install-customizations-cloud">während der Installation</a> kein SSH-Keyfile angegeben wurde, kann anschliessend nicht auf die VM's per SSH zugegriffen werden.</li>
             <p>Die Installation eines Clusters dauert je nach Plattform unterschiedlich lang:</p>
             <img src="/images/blogposts/Installationsdauer.png" alt="Installationsdauer"/>
             <h4>VMware vSphere</h4>
             <p>Einige Bemerkungen zur VMware vSphere Installation:</p>
             <li>Die Dokumentation zur Installation war sehr gut und wir konnten dieser Schritt für Schritt folgen.</li>
             <li>Während der (ersten) Installation mussten wir feststellen, dass Reverse DNS Einträge zwingend notwendig sind. Die Installation war blockiert und wir mussten von neuem beginnen.</li>
             <li>Kleine Fehler in den Ignition (JSON) Files führen zu Fehler, die leider sehr schwer zu finden sind, da keine sinnvolle Fehlermeldung vorhanden ist. So hat uns z.B. ein fehlendes Komma etwa eine Stunde Zeit gekostet. Ignition Files können <a href="https://coreos.com/validate/">hier</a> validiert werden.</li>
             <li>Für die Installation muss ein Load Balancer (z.B. HAproxy) erstellt werden. Siehe auch unten im Teil zu Networking.</li>
             <li>Infrastructur MachineSets für vSphere sind noch nicht implementiert. Daher ist die Installation auf VMware vSphere auch eine <a href="https://blog.openshift.com/openshift-4-install-experience/">User Provisioned Infrastructure (UPI)</a> Installation.</li>
             <p><b>GCP</b></p>
             <p>Damit die Installation auf GCP funktioniert, müssen die folgenden APIs aktiviert sein:</p>
             <li>Identity and Access Management(IAM)</li>
             <li>Cloud Resource Manager API</li>
             <p>Weiter muss die Disksize Limit von 500GB auf 750 GB erh&ouml;ht werden (640GB benutzt nach der Installation). Das Definieren im <em>install-config.yaml</em> File des OpenShift Installer funktioniert leider nicht:</p>
             <pre class="western">
             platform:
                gcp:
                   rootVolume:
                   iops: 4000
                   size: 50
                   type: io1
                type: n1-standard-4</pre>
             <p><b>Azure</b></p>
             <p>Einige Bemerkungen zur Installation auf Azure:</p>
             <li>Free Tier Subscription reicht nicht aus für eine OpenShift Installation.</li>
             <li>Es müssen Anpassungen an den default Azure Account Limits gemacht werden.</li>
             <li>Zürich befindet sich z.Z. nicht unter den supported Azure Regions.</li>
             <li>Die Dokumentation ist falsch bzgl. <em>Azure account limits & Creating a service principal</em>.</li>
             <p><b>AWS</b></p>
             <p>Die Installation auf AWS war am einfachsten. Das liegt wohl daran, dass bereits Openshift 4.0 darauf ausgelegt war.</p>
             <h4>Networking</h4>
             <img src="/images/blogposts/networking.png" alt="networking"/>
             <p><b>Load Balancing</b></p>
             <p>Für eine RedHat OpenShift 4.2 Installation werden zwei Load Balancer vor dem Cluster benötigt:</p>
             <li><b>API Load Balancer:</b> Für eine Hochverfügbarkeit der Kubernetes-API (welche auf dem Master läuft), müssen alle API Calls an diese Master Nodes verteilt werden.</li>
             <p>Auf den drei Cloud Provider konnten wir dafür jeweils den Load Balancer Service des Providers verwenden. Dieser wird mit dem OpenShift Installer automatisch konfiguriert. Auf VMware vSphere mussten wir dafür selber einen Load Balancer konfigurieren. Wir haben dafür eine CentOS VM mit HAProxy verwendet. Für die hohe Verfügbarkeit kann z.B. keepalived verwendet werden.</p>
             <li><b>Client Access Load Balancer:</b> Für den Zugriff auf den Applikation Workload wird ein Load Balancer benötigt, der die Ingress Controller weiterleitet.</li>
             <p>Für den Client Access Load Balancer kann auf die Cloud Provider Integration von Kubernetes zurückgegriffen werden. Damit werden mit Hilfe eines <a href="https://kubernetes.io/docs/concepts/services-networking/service/#
             lancer">Kubernetes Services vom Typ Load Balancer</a> automatisch ein Load Balancer auf der entsprechenden Cloud Plattform provisioniert.</p>
             <p>Bei der on-premise Installation mit VMware vSphere muss der Load Balancer selber implementiert werden, welcher den Netzwork Traffic auf die Ingress Controller weiterleitet. Das automatische Erstellen des Load Balancers via Kubernetes Service funktioniert hier leider nicht.</p>
             <h4>Egress Traffic & NetworkPolicy</h4>
             <li><b>NetworkPolicy:</b>Die Kubernetes v1 NetworkPolicy Features sind in OpenShift 4.2 verfügbar</li>
             <li><b>Egress IP:</b> Identisch zu OpenShift 3.11. <a href="https://docs.openshift.com/container-platform/4.2/networking/openshift-sdn/assigning-egress-ips.html">(Referenz)</a></li>
             <li><b>EngressNetworkPolicy</b> wir auf OpenShift 4.2 nicht unterstützt </li>
             <li><b>EgressRouter</b> wird auf OpenShift 4.2 nicht unterstützt </li>
             <h4>Storage</h4>
             <img src="/images/blogposts/storage.jpg" alt="storage"/>
             <p>Abschliessend schauen wir uns noch die diversen Storage Integrationen für OpenShift 4.2 an. Wir teilen Storage in drei Kategorien ein: Block Storage, File Storage und Object Storage.</p>
             <p><b>Black Storage</b></p>
             <p>Erfreulicherweise hatten wir mit keinem Provider (onpremise wie auch Cloud) Probleme. Nach der Installation gemäss Anleitung konnten wir Block Storage von allen Cloud Providern beziehen. Alle Infos dazu sind in der entsprechenden Dokumentation von RedHat zu finden.</p>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-gce.html">GCP</a></li>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-azure.html">Azure</a></li>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-aws.html">AWS</a></li>
             <li><a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-vsphere.html">vSphere</a></li>
             <p><b>File Storage</b></p>
             <p>Als File Storage bezeichnen wir solchen, der insbesondere shared bezogen werden kann <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">(ReadWriteMany - RWX)</a>.</p>
             <p><b>AWS</b></p>
             <p>Auf AWS steht uns <a href="https://aws.amazon.com/efs/">EWS</a> zur Verfügung, dabei haben wir jedoch Folgendes festgestellt:</p>
             <li>Auf EFS sind alle Volumes nur Subfolder des Root Volume.</li>
             <li>Quotas können nicht forciert werden.</li>
             <li>Keine Usage Metrics</li>
             <li><em>size</em> in einem PVC wird nicht berücksichtigt.</li>
             <li>RedHat sagt zu EFS: <em>"Elastic File System is a Technology Preview feature only.[...]"</em> <a href="https://docs.openshift.com/container-platform/4.2/storage/persistent-storage/persistent-storage-efs.html">Openshift doc on EFS</a></li>
             <li>upstream efs-provisioner: <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs">detailed doc & code</a></li>
             <p>Weiter kann auch Netapp Trident verwendet werden. In unserer /mid Week haben wir dies jedoch nicht angeschaut (auch nicht auf den anderen Cloud Provider Plattformen). Dafür wird ein AWS Account mit konfiguriertem NetApp CVS (1TB / 100$ / Monat) benötigt. Infos dazu in der <a href="https://netapp-trident.readthedocs.io/en/stable-v19.07/kubernetes/operations/tasks/backends/cvs_aws.html">Netapp Trident Dokumentation</a>.</p>
             <p><b>Azure</b></p>
             <p>Microsoft bietet mit Azure File die Möglichkeit, dynamisch File Storage zu beziehen. Die folgenden Features werden dabei aber nicht supported.</p>
             <li>Symlinks</li>
             <li>Hard links</li>
             <li>Extended attributes</li>
             <li>Sparse files</li>
             <li>Named pipes</li>
             <p>Alle Informationen dazu sind in der <a href="https://docs.openshift.com/container-platform/4.2/storage/dynamic-provisioning.html#azure-disk-definition_dynamic-provisioning">RedHat OpenShift Dokumentation</a> zu finden. Auch auf Azure kann <a href="https://netapp-trident.readthedocs.io/en/stable-v19.07/kubernetes/operations/tasks/backends/anf.html">Netapp Trident</a> verwendet werden.</p>
             <p><b>GCP</b></p>
             <p>Auf GCP stehen <a href="https://cloud.google.com/solutions/filers-on-compute-engine">mehrere Möglichkeite</a>n zur Verfügung.</p>
             <p>Für <a href="https://cloud.google.com/community/tutorials/gke-filestore-dynamic-provisioning">Cloud File Store kann der nfs-client-provisioner</a> verwendet weren. Auch hier können Quotas nicht forciert werden. Weiter gibt es einen nicht offiziell supporteten <a href="https://github.com/kubernetes-sigs/gcp-filestore-csi-driver">CSI Treiber</a>, den wir aber nicht wirklich zum Laufen gebracht haben.</p>
             <p>Weiter kann auch hier NetApp Cloud Volumes mit Trident verwendet werden. Wir haben hierzu aber keine Dokumentation gefunden. Dies sollte aber ähnlich wie bei <a href="https://netapp-trident.readthedocs.io/en/stable-v19.04/kubernetes/operations/tasks/backends/cvs_aws.html">AWS</a> seine. Weitere Infos dazu sind <a href="https://cloud.google.com/solutions/filers-on-compute-engine#netapp">hier</a> zu finden.</p>
             <p>Eine weitere Möglichkeit ist die Verwendung von <a href="https://github.com/Elastifile/elastifile-provisioner">Elastifile</a> oder <a href="https://github.com/quobyte/quobyte-csi">Quoabyte</a>. Diese müssen aber alle lizenziert werden. Elastifile ist nur für einen eingeschränkten Kundenkreis verfügbar. Quoabyte sieht zur Zeit noch nicht wirklich Enterprise-like aus.</p>
             <p><b>VMware vSphere</b></p>
             <p>In einer on-premise VMware vSphere Umgebung muss einen File Storage Service selbst aufgebaut werden (z.B. mit <a href="https://www.openshift.com/products/container-storage/">RedHat Contrainer Storage</a> basierend auf <a href="https://rook.io/">rook.io</a>).</p>
             <p><b>Object Storage</b></p>
             <p>Auch Object Storage wird von den drei Cloud Provider angeboten. Dieser wird aber in der Regeln nicht als PV in einen Pod gemounted, sondern direkt aus einer Applikation bezogen. Der Vollständigkeit halber hier eine Auflistung der Object Storage Services.</p>
             <li>GCP mit <a href="https://cloud.google.com/storage/">Google Cloud Storage</a></li>
             <li>AWS mit <a href="https://aws.amazon.com/s3/">S3</a></li>
             <li>Azure mit <a href="https://azure.microsoft.com/en-us/services/storage/">Azure Object Storage</a></li>
             <p>In einer on-premise VMware vSphere Umgebung muss ein Object Storage Service selbst aufgebaut werden.</p>

- day: 09
  month: Dez
  title: "Quarkus on APPUiO"
  subtitle: |
            <img src="/images/blogposts/Quarkus_on_APPUiO.png" alt="Quarkus on APPUiO"/>
            With the release of Quarkus 1.0.0, Java has once again become attractive for microservices. In this blog post I show you how to run native applications on APPUiO using Quarkus. This is a guest article by <a href="https://nxt.engineering/en/team/mig/">Michael Gerber</a> form <a href="https://nxt.engineering/en/">nxt Engineering GmbH</a>.
  details: |
            <h4>Create a Quarkus Project</h4>
            <p>The easiest starting point to create a <a href="https://quarkus.io/">Quarkus</a> project is the website <a href="https://code.quarkus.io">https://code.quarkus.io</a>. The same concept is already known from Spring Boot <a href="https://start.spring.io/">https://start.spring.io</a>. This makes it easy to create a project with the needed libraries.</p>
            <img src="/images/blogposts/Quarkus_APPUiO_1.png" alt="Quarkus APPUiO 1"/>
            <p>For this example choose the extensions <em>RESTEasy JAX-RS</em> and <em>SmallRye Health</em>. RESTEasy JAX-RS, which is included as default, is used for REST web services and SmallRye Health is an extension that helps to integrate health checks.</p>
            <h4>Start the Application</h4>
            <p>To start Quarkus locally you just need to have Java 8 installed. With the command <em>./mvnw compile quarkus:dev</em> you can start the application in development mode. Changes in the Java code are applied directly in the running application via hot deployment. Thus, a time-consuming application restart is not necessary 😃.</p>
            <h4>Create a REST Web Service</h4>
            <p>As an example application, we develop a REST web service that calculates a specific number in the <a href="https://en.wikipedia.org/wiki/Fibonacci_number">Fibonacci sequence</a>. The sequence is named after <a href="https://en.wikipedia.org/wiki/Fibonacci">Leonardo Fibonacci</a>, who described the growth of a rabbit population in 1202. The Fibonacci sequence can be calculated recursively as well as iteratively. To push the CPU a little bit, I use the recursive variant.</p>
            <pre><code>package nxt;
            
            import javax.ws.rs.GET;
            import javax.ws.rs.Path;
            import javax.ws.rs.PathParam;
            import javax.ws.rs.Produces;
            import javax.ws.rs.core.MediaType;
            
            @Path("/fibonacci")
            public class FibonacciResource {
            
                @GET
                @Produces(MediaType.TEXT_PLAIN)
                @Path("{number}")
                public int fibonacci(@PathParam("number") Integer number) {
                    if (number == 0) {
                            return 0;
                    }
                    if (number == 1) {
                            return 1;
                    }
                    return fibonacci(number - 2) + fibonacci(number - 1);
                }
            }
            </code></pre>
            <p>The application can be tested with curl as follows:</p>
            <pre><code>curl http://0.0.0.0:8080/fibonacci/10</code></pre>
            <h4>Test a REST Web Service</h4>
            <p>Quarkus uses the well-known library <a href="http://rest-assured.io/">REST-assured</a> for REST web service tests. The web service created earlier can be tested with the following code:</p>
            <pre><code>package nxt;
            
            import io.quarkus.test.junit.QuarkusTest;
            import org.junit.jupiter.api.Test;
            
            import static io.restassured.RestAssured.given;
            import static org.hamcrest.CoreMatchers.is;
            
            @QuarkusTest
            public class FibonacciResourceTest {
            
                @Test
                public void testEndpoint() {
                   testFibonacci(0, 0);
                   testFibonacci(1, 1);
                   testFibonacci(2, 1);
                   testFibonacci(3, 2);
                   testFibonacci(4, 3);
                   testFibonacci(5, 5);
                   testFibonacci(6, 8);
                }
                private void testFibonacci(int number, int fibonacci) {
                   given()
                      .when().get("/fibonacci/"+ number)
                      .then()
                        .statusCode(200)
                        .body(is(String.valueOf(fibonacci)));
                }
            }
            </code></pre>
            <p>The test can be executed with the command <em>./mvnw test.</em></p>
            <h4>Build Application on APPUiO</h4>
            <p>For the application to run on APPUiO, a Swiss OpenShift platform, it must first be built. The Quarkus team provides an OpenShift Source-to-Image (S2I) build.</p>
            <p>With the following command you can create a new build on OpenShift and start it automatically:</p>
            <pre><code>oc new-build quay.io/quarkus/ubi-quarkus-native-s2i:19.2.1~https://gitlab.com/nxt/public/quarkus-fibonacci.git \
              --name=quarkus-fibonacci-build
            </code></pre>
            <p>The <a href="https://www.graalvm.org/">GraalVM</a>, which creates a native image from the Java code, needs a lot of computing power. In order for the build on APPUiO to acquire the needed resources, it has to be configured appropriately.</p>
            <pre><code>oc patch bc/quarkus-fibonacci-build \
              -p '{"spec":{"resources":{"requests":{"cpu":"0.5", "memory":"2Gi"},"limits":{"cpu":"4", "memory":"4Gi"}}}}'
            </code></pre>
            <p>The above command sets the build job limit to 4 CPUs and 4 gigabytes of RAM.</p>
            <p>The great thing about APPUiO is that the resources needed for the build are not pulled from the project’s own resources. So you can push your own project to the limit and still run builds on OpenShift 😎.</p>
            <p>The disadvantage of this approach is that the resulting docker image has a size of 600 MB 🤔. This is because the docker image contains the entire <em>GraalVM</em>. A Docker multistage build can solve this problem nicely.</p>
            <p>The following docker file contains a multistage build where the native image is built first with <em>GraalVM</em> and after that a minimal docker image based on <em>ubi-minimal is built</em>.</p>
            <pre><code>## Stage 1 : build with maven builder image with native capabilities
            FROM quay.io/quarkus/centos-quarkus-maven:19.2.1
            COPY src /usr/src/app/src
            COPY pom.xml /usr/src/app
            USER root
            RUN chown -R 1001 /usr/src/app
            USER 1001
            RUN mvn -f /usr/src/app/pom.xml package -Pnative -e -B -DskipTests -Dmaven.javadoc.skip=true -Dmaven.site.skip=true -Dmaven.source.skip=true -Djacoco.skip=true -Dcheckstyle.skip=true -Dfindbugs.skip=true -Dpmd.skip=true -Dfabric8.skip=true -Dquarkus.native.enable-server=true
            
            ## Stage 2 : create the docker final image
            FROM registry.access.redhat.com/ubi8/ubi-minimal
            WORKDIR /work/
            COPY --from=0 /usr/src/app/target/*-runner /work/application
            RUN chmod 775 /work
            EXPOSE 8080
            CMD ["./application", "-Dquarkus.http.host=0.0.0.0"]</code></pre>
            <p>On OpenShift, you can run multistage docker builds. The first of the following two commands creates the image stream for the new lean docker image and the second one creates the docker build.</p>
            <pre><code>oc create is quarkus-fibonacci
            oc create -f - | << EOF
            {
                "apiVersion": "build.openshift.io/v1",
                "kind": "BuildConfig",
                "metadata": {
                    "labels": {
                         "build": "quarkus-fibonacci-build"
                    },
                    "name": "quarkus-fibonacci-build"
                },
                "spec": {
                    "output": {
                        "to": {
                            "kind": "ImageStreamTag",
                            "name": "quarkus-fibonacci:latest"
                        }
                    },
                    "resources": {
                        "limits": {
                             "cpu": "4",
                             "memory": "4Gi"
                         },
                         "requests": {
                             "cpu": "500m",
                             "memory": "2Gi"
                         }
                    },
                    "source": {
                         "git": {
                             "uri": "https://gitlab.com/nxt/public/quarkus-fibonacci.git"
                         },
                         "type": "Git"
                    },
                    "strategy": {
                         "type": "Docker"
                    }
                }
            }
            EOF
            </code></pre>
            <h4>Publish Application to APPUiO</h4>
            <p>The previously built application can be published on APPUiO with the following two commands:</p>
            <pre><code>oc new-app --image-stream=quarkus-fibonacci:latest
            oc expose svc quarkus-fibonacci
            </code></pre>
            <p>The first command creates a deployment with a Pod and a corresponding service. The second command creates a route for the given service that makes the microservice available to the public.</p>
            <p>The command below allows you to test the newly deployed application:</p>
            <pre><code>curl http://$(oc get route | grep quarkus-fibonacci | awk '{print $2}')/fibonacci/1</code></pre>
            <h4>Set up Health Checks</h4>
            <p>Quarkus has an extension that offers health checks out of the box. If the extension has not yet been added to the project, it can be added with the command <em>./mvnw quarkus:add-extension -Dextensions="health"</em>. Quarkus will then automatically create health check endpoints that can be called via the URL <em>/health/live</em> and <b>/health/ready</b>.</p>
            <p>In OpenShift the health checks can be added with the following command:</p>
            <pre><code>oc set probe dc/quarkus-fibonacci --liveness --get-url=http://:8080/health/live --initial-delay-seconds=1
            oc set probe dc/quarkus-fibonacci --readiness --get-url=http://:8080/health/ready --initial-delay-seconds=1</code></pre>
            <h4>Autoscaling</h4>
            <p>Quarkus applications can be started extremely quickly. This feature is especially useful when using the autoscaling feature. This allows you to start and stop pod’s dynamically when needed.</p>
            <pre><code>oc autoscale dc/quarkus-fibonacci --min 1 --max 2 --cpu-percent=80</code></pre>
            <p>The command above adds autoscaling, which starts a second Pod when needed. As soon as the load decreases again, the additional Pod is automatically shut down again.</p>
            <p>Autoscaling can be tested with the ApacheBench tool from Apache.</p>
            <pre><code>ab -n 5000 -c2 http://$(oc get route | grep quarkus-fibonacci | awk '{print $2}')/fibonacci/30</code></pre>
            <p>This command sends 5000 parallel requests to the microservice. After a few seconds OpenShift will start the second Pod, which will be ready to use immediately.</p>
            <img src="/images/blogposts/quarkus_APPUiO_2.png" alt="Quarkus on APPUiO 3"/>
            <h4>Conclusion</h4>
            <p>With Quarkus you can quickly and easily build a microservice with Java, which meets all requirements to be operated efficiently in an OpenShift or Kubernetes.</p>
            <p>You can study <a href="https://gitlab.com/nxt/public/quarkus-fibonacci">the complete code on in GitLab repository</a>.</p>
            
            
            
            
            
- day: 17
  month: Sept
  title: "Red Hat Forum 2019"
  subtitle: |
            <img src="/images/blogposts/APPUiO_Red-Hat-Forum_2019.jpg" alt="Red Hat Forum 2019"/>
            Vor gut einer Woche fand das 8. Red Hat Forum in Zürich Oerlikon statt. Auch dieses Jahr war APPUiO als Sponsor mit einem Stand vertreten. Über 800 Teilnehmer warteten gespannt auf einen intensiven und spannenden Tag.
  details: |
            <h4>Red Hat und IBM</h4>
            <p>Die Übernahme von Red Hat durch IBM war neben den Open Source-, DevOps-, Microservices- und Container-Themen ein zentraler Schwerpunkt. Nach Meinungen des Managements wie auch der einzelnen Mitarbeitern, sei IBM eine grosse Chance und ein «Shareholder sowie Partner gleichzeitig». Es sei eine Gelegenheit, die neue Türen öffnet. Von der Beständigkeit Red Hat's wie sie heute ist, sind sie dennoch überzeugt.</p>
            <img src="/images/blogposts/Red-Hat-Forum_Hut.jpg" alt="Red Hat Forum"/>
            <h4>APPUiO am Red Hat Forum</h4>
            <p>APPUiO war mit einem eigenen Stand vor Ort vertreten. Mit dabei natürlich: Ein gelber Container und das APPUiO-Team. Für das Team stand der Austausch mit den Besuchern im Vordergrund. Nicht nur neue Kontakte konnten geknüpft werden, hin und da besuchte ein bekanntes Gesicht den APPUiO-Stand. Spannende Keynotes und Breakout-Sessions liessen den tollen Tag abrunden.</p>
            <img src="/images/blogposts/APPUiO_Red-Hat-Forum_Retro.jpg" alt="APPUiO Retro"/>
            <p>Unser neuestes Gemeinschaftsprojekt. APPUiO on Philips. ;-) </p>
            <h4>APPUiO beerup</h4>
            <p>Was wäre ein solch gelungener Tag bereits schon zu Ende? Das dachte sich APPUiO auch. Deshalb lud APPUiO die Besucher zu einem beerup in die Giesserei ein. Der Besucherdrang war noch grösser als beim ersten beerup im Sihlcity – ein richtiger Erfolg. In ungezwungener Atmosphäre wurde ein Bierchen getrunken, über dies und jenes gesprochen und den Tag ausgeklungen.</p>
            <img src="/images/blogposts/APPUiO_beerup.jpg" alt="APPUiO beerup"/>
            <p>Aber nur wer in Besitz eines Golden-Tickets war, wurde von der strengen Einlasskontrolle hereingelassen.:-)</p>
            <img src="/images/blogposts/goldenticket.jpg" alt="APPUiO goldenticket"/>
            <p>Wir freuen uns schon auf das nächste Treffen, bis dahin: Macht's gut!</p>
            <p>Euer APPUiO-Team</p>

- day: 29
  month: Aug
  title: "OpenShift 4"
  subtitle: |
            <img src="/images/blogposts/OpenShift4.png" alt="OpenShift4"/>
            Seit Juni dieses Jahres ist OpenShift 4.1 verfügbar, der erste öffentlich zugängliche Release von Red Hat (Version 4.0 war ein rein interner Release).
            Wir möchten dir mit einer Blogpost-Serie Informationen, Erfahrungsberichte, Empfehlungen sowie Tipps und Tricks weitergeben, damit du frühzeitig über die nötigen Informationen verfügst. Zusätzlich werden wir verschiedene Events wie beerups oder Techtalks organisieren, damit du detailliertere und technischere Berichte erhältst.
            Falls du in deinem Unternehmen Unterstützung benötigst oder wir dir mögliche Wege zu OpenShift 4 aufzeigen sollen, darfst du dich gerne bei uns melden.
            Starten wir mit ein paar Grundlagen zu OpenShift 4.
  details: |
            <h4>Entwickler-Tools</h4>
            <p>Mit OpenShift 4 ändert sich viel und doch nicht, zumindest aus Entwicklersicht. Die Verwendung von OpenShift 4 wird nichts bis fast nichts ändern. Um das Leben eines Entwicklers zu vereinfachen, hat Red Hat ein Tool <code> odo </code> entwickelt, welches nur die für Entwickler relevanten <code> oc-</code>Befehle enthalten wird. Ausserdem wird mit dem <a href="https://developers.redhat.com/products/codeready-workspaces/overview">Red Hat CodeReady Workspaces</a> eine "Kubernetes-native developer workspace server and IDE" zur Verfügung gestellt, um das Entwickeln von auf OpenShift lauffähigen Applikationen zu vereinfachen.</p>
            <h4>Installation</h4>
            <p>Die Installation von OpenShift 4 wurde stark vereinfacht. Mit dem einfachen Befehl <code>openshift-install create cluster</code> kann der Installationsassistent gestartet werden. Ohne weiteres Zutun fragt dieser die benötigten Konfigurationsparameter ab, die er nicht selbst herausfinden kann. Für alles andere werden vernünftige Defaults verwendet und so automatisch die Referenzarchitektur eingehalten.</p>
            <p>Die Control Plane Hosts werden immer mit Red Hat CoreOS (RHCOS) aufgesetzt, bei den restlichen besteht die Wahl zwischen RHCOS und klassischem RHEL. Der Vorteil der Verwendung von RHCOS besteht darin, dass OpenShift das Betriebssystem selbst verwalten und somit auch automatisch aktualisieren kann.</p>
            <p>Auf unterstützten Plattformen ist der Installer in der Lage, die gesamte zugrundeliegende Infrastruktur selbst zu provisionieren. Diese Art von Installation wird mit IPI (Installer Provisioned Infrastructure) bezeichnet und stellt die empfohlene Installationsvariante dar. Bei der anderen Art von Installation, UPI (User Provisioned Infrastructure), wird die Infrastruktur, wie es der Name bereits andeutet, selbst aufgebaut und dem Installer zur Verfügung gestellt.</p>
            <p>Updates können neu über die Web Console durchgeführt werden, wobei zwischen drei Channels (stable, pre-release, nightly) ausgewählt werden kann.</p>
            <p>Schaut man unter die Haube von OpenShift 4, fällt auf, dass nebst der Kernkomponente Kubernetes die Container Engine ausgewechselt wurde. Anstelle von Docker kommt neu <a href="https://cri-o.io/">CRI-O</a> zum Einsatz. CRI-O verwendet als darunter liegende Container Runtime <code>runc</code>, wie dies auch Docker tut, und ist komplett <a href="https://www.opencontainers.org/">OCI</a>-compliant. Beispielsweise können mit Docker gebaute Images auch mit CRI-O problemlos gestartet werden - also kein Grund zur Sorge in dieser Hinsicht. Einer der Gründe für diesen Wechsel war, den monolithischen Docker-Daemon in einzelne Tools mit jeweils einem bestimmten Zweck aufzuteilen, ganz gemäss der Unix-Philosophie. So wurden nebst CRI-O die Container Tools <a href="https://buildah.io/">buildah</a>, <a href="https://podman.io/">Podman</a> und <a href="https://github.com/containers/skopeo">skopeo</a> ins Leben gerufen und sind schon seit einiger Zeit verfügbar.</p>
            <h4>Update von OpenShift 3 auf 4</h4>
            <p>Das Wichtigste vorweg: Es wird kein Update-Pfad von OpenShift 3 auf 4 geben. Red Hat stellt aber ein Migrations-Tool zur Verfügung, welches nicht nur die Kubernetes Ressourcen, sondern sogar die Daten von Persistent Volumes migrieren kann. Dabei wird S3 Storage als Zwischenspeicher verwendet. Das Migrations-Tool unterstützt neben Migrationen von Version 3 auf 4 auch Migrationen zwischen unterschiedlichen v4 Clustern.</p>
            <h4>Operators</h4>
            <p>Dass Operators ein wesentlicher Bestandteil von OpenShift 4 sein werden, ist bereits weitherum bekannt. Was Operators aber genau sind, wohl noch weniger: Ein Operator ist eine Methode für die Paketierung, das Deployment sowie die Verwaltung von Kubernetes-nativen Applikationen. Eine Kubernetes-native Applikation ist eine Applikation, welche sowohl auf Kubernetes deployt wie auch über die Kubernetes API verwaltet werden kann.</p>
            <p>Ein Operator ist grundsätzlich ein Custom Controller, wobei der Controller zu den Kernkonzepten von Kubernetes gehört. Er vergleicht regelmässig den gewünschten mit dem effektiven Zustand einer oder mehrerer Ressourcen auf dem Cluster und korrigiert diesen falls nötig. Ähnlich wie dies auch Puppet tut. Ein Operator selbst läuft als Pod auf dem Cluster.</p>
            <p>Operators übernehmen in OpenShift 4 eine zentrale Rolle. Sie sind für die Steuerung und Überwachung von so ziemlich jeder einzelnen Komponente verantwortlich, darunter auch kritische Netzwerk- und Credential-Dienste. Wiederum ein Operator übernimmt die Verwaltung aller dieser Operators, der sog. Cluster Version Operator. Nebst diesen vom Cluster Version Operator verwalteten Plattform-Operators können auch Applikationen Gebrauch vom Operator Framework machen. Sie werden allerdings nicht vom Cluster Version Operator, sondern vom Operator Lifecycle Manager (OLM) verwaltet. Eine Übersicht verfügbarer Operators ist auf <a href="https://operatorhub.io/">OperatorHub.io</a> ersichtlich. Analog zu bspw. dem in Rancher integrierten App Catalog für 
            Charts, ist auch der OperatorHub in OpenShift 4 integriert und ermöglicht eine einfache, grafische Installation über die Web Console.</p>
            <h4>Operator Framework</h4>
            <p>Operators können aber auch selbst geschrieben werden, bspw. mithilfe des Operator Frameworks. Das Framework unterstützt Helm, Ansible sowie Go, wobei jede Variante natürlich seine eigenen Vor- und Nachteile hat:</p>
            <li>Helm ist sehr einfach zu schreiben, da kein Code geschrieben werden muss. Zudem wird auch kein Tiller mehr benötigt.</li>
            <li>Ansible ist für Betreiber die erste Wahl, da meist bereits Ansible-Know How vorhanden ist.</li>
            <li>Go stellt zwar die vermutlich herausforderndste, dafür, aufgrund seiner vollwertigen Programmiersprache, aber auch die mächtigste und flexibelste Variante dar.</li>
            <p>Der sog. Capability Level dieser Varianten wird in folgender Abbildung aufgezeigt. Der Capability Level veranschaulicht, welche Phasen unterstützt werden.</p>
            <img src="/images/blogposts/operator-capability-level-transparent-bg.png" alt="Capability level"/>
            <h4>OpenShift Container Storage</h4>
            <p>Red Hat beschreibt den OpenShift Container Storage, oder kurz RHOCS oder OCS, als "Add-On for OpenShift for running stateful apps". Während OCS unter OpenShift 3 noch aus Gluster und Heketi bestand, um dynamisch Persistent Volumes zu allozieren, soll diese Aufgabe die Kombination aus <a href="https://rook.io/">Rook</a>, <a href="https://ceph.io/">Ceph</a> und <a href="https://www.noobaa.io/">NooBaa</a> übernehmen. Gründe für diesen Wechsel seien insbesondere das beachtliche Momentum hinter der Entwicklung von Rook sowie der aus Sicht Red Hat zunehmende Fokus auf Object Storage, welcher von NooBaa in Form einer S3-kompatiblen API abgedeckt wird. Wie auch schon bei Version 3 werden ein Independent sowie ein Converged Mode angeboten. OCS kann also als externer Storage Cluster oder aber direkt auf OpenShift selbst installiert werden. Anders als beim Gluster-Heketi-Stack soll neu der Storage via CSI (Container Storage Interface) angebunden werden können.</p>
            <p>Geplant ist, OpenShift Container Storage 4 mit OCP Version 4.2 zu veröffentlichen, welches wiederum in Q3 2019 geplant ist. OpenShift Container Storage 3.X soll noch bis Juni 2022, gleich wie OpenShift Container Platform 3.X, <a href="https://access.redhat.com/support/policy/updates/openshift/">supportet sein</a>. Wie bereits bei OpenShift 3 wird RHOCS auch auf v4 durch die OpenShift Storage Addon-Subscription abgedeckt. Wer also bereits im Besitz einer solchen ist, ist mit OpenShift 4 bereits abgedeckt.</p>
            <h4>Service Mesh</h4>
            <p>Das OpenShift Service Mesh besteht nicht nur, wie häufig angenommen, aus <a href="https://istio.io/">Istio</a>, sondern zudem aus <a href="https://www.kiali.io/">Kiali</a>, <a href="https://www.jaegertracing.io/">Jaeger</a> sowie <a href="https://www.envoyproxy.io/">Envoy Proxy</a>. Das Service Mesh ermöglicht besseres Tracking und Management der Kommunikation zwischen Services und Pods, indem ein Envoy Proxy als Sidecar-Container in die Pods hinzugefügt wird. Envoy stellt dabei die Data Plane innerhalb des Service Mesh dar. Die Control Plane (nicht zu verwechseln mit der OpenShift Control Plane) ist verantwortlich für die Verwaltung und Konfiguration der Envoy Proxies um den Traffic zu routen wie auch Policies anzuwenden. Diese Konstellation ergibt ein Netzwerk von Services mit Load Balancing, Service-zu-Service-Authentifizierung, Monitoring und mehr. Code-Änderungen an der Applikation sind dabei keine bis nur wenige notwendig.</p>
            <h4>Pipelines</h4>
            <p>OpenShift Pipelines setzt auf <a href="https://tekton.dev/">Tekton</a>, welches vorher unter dem Namen Knative Build-Pipeline bekannt war. Die Idee dahinter ist, cloud-native CI/CD unter Kubernetes zur Verfügung zu stellen. Dabei werden die verschiedenen für eine Pipeline benötigten Komponenten (wie auch die Pipeline selbst) als Custom Resources angelegt und können so via <code>kubectl</code>/<code>oc</code> administriert werden. Gem. Roadmap soll mit Version 4.2 ein Tech Preview und mit 4.3 die GA-Version erhältlich sein.</p>
            <h4>Serverless</h4>
            <p>Auch "serverless" darf natürlich nicht fehlen, welches mit <a href="https://knative.dev">Knative</a> realisiert wird. Gemäss Roadmap soll mit Version 4.2 ein Tech Preview und mit 4.3 die GA-Version erhältlich sein. Was FaaS bedeutet kannst du im <a href="https://appuio.ch/blog.html#2017-Okt-10">Blogpost</a> von Tobru nachlesen.</p>
            <h4>Schlusswort</h4>
            <p>Die aufgeführten Neuerungen sind natürlich nicht abschliessend, daher kann sich ein Blick in die <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.1/html-single/release_notes/index">Release Notes von OpenShift 4.1</a> oder in einen der vielen anderen Artikel, Blogposts etc. lohnen. Mit OpenShift 4.2 wird der wohl erste Release veröffentlicht, der die meistgefragten Features und Unterstützungen mitbringt. Ein <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.1/html-single/release_notes/index">kurzer Blick</a> lohnt sich also auf alle Fälle. Wir von APPUiO werden weiter Erfahrung mit dem brandneuen OpenShift-Release, auch in Zusammenarbeit mit unseren Kunden, sammeln und in eine bestmögliche Unterstützung umsetzen.</p>       

- day: 26
  month: Aug
  title: "10 wissenswerte Fakten zu Kubernetes und OpenShift"
  subtitle: |
            <img src="/images/blogposts/Kubernetes_OpenShift.png" alt="Kubernetes und OpenShift"/>
            Bei APPUiO setzen wir auf OpenShift von Red Hat. OpenShift ist eine Kubernetes Distribution. Kubernetes ist eine Plattform zur Orchestrierung von Container-Systemen. Alles klar? Bist du noch dabei? :-) Wenn ja, dann lies weiter und erfahre im folgenden Blogpost, was Kubernetes, eine Kubernetes Distribution und OpenShift ist und warum wir APPUiO auf den Grundlagen von Kubernetes und OpenShift aufgebaut haben.
  details: |
            <h4>1. Was ist Kubernetes?</h4>
            <p>Kubernetes ist die Plattform zur Orchestrierung von Container-Systemen. Kubernetes automatisiert das Einrichten, Betreiben und auch das Skalieren von containerisierten Anwendungen. Die Open Source Plattform wird auch mit "K8s" abgekürzt und das Wort Kubernetes kann mit "Steuermann" übersetzt werden.</p>
            <p>Zu den Funktionen von Kubernetes zählen unter anderem die Automatisierung von Containern und des Software Rollouts, die Optimierung der eingesetzten Ressourcen, Persistent Storage, Service Discovery, Autoscaling und HA. Im Vergleich zu anderen Orchestrierungsplattformen, wie bspw. Docker Swarm, unterstützt Kubernetes auch andere containerbasierte Virtualisierungssysteme.</p>
            <p>Die offizielle Beschreibung von Kubernetes lautet:</p>
            <p><em>Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation.</em></p>
            <p>Für das Verständnis dieses Beitrags ist es wichtig zu verstehen, dass es sich bei Kubernetes um eine Plattform handelt und nicht um ein fixfertiges Produkt ab der Stange.</p>
            <h4>2. Wer steckt hinter Kubernetes und wer entwickelt es weiter?</h4>
            <p>Kubernetes wurde ursprünglich von Google entwickelt, um die benötigte riesige Infrastruktur für die Suchmaschine bereitzustellen und zu optimieren. Durch den Einsatz von Virtualisierung von Hardware, Cloud Computing, Site Reliability Engineering und Container-Technologien führte die Lösung dazu, dass die bestehende Infrastruktur viel besser ausgelastet wurde. Dies führte zu wesentlich geringeren Kosten der Infrastruktur-Landschaft. Zum Durchbruch verhalf Google Kubernetes aber dadurch, dass Kubernetes als Open Source Lösung zur Verfügung gestellt und an die Cloud Native Computing Foundation (CNCF) gesoendet wurde.</p>
            <p>Heute zählt Kubernetes zu den aktivsten Open Source Projekten weltweit und die <a href="https://kubernetes.io/partners/#kcsp">Partner-Landschaft</a> liest sich wie das Who-is-Who der IT-Welt. </p>
            <h4>3. Was ist eine Kubernetes Distribution?</h4>
            <p>Um die Unterschiede von Kubernetes und OpenShift zu verstehen, muss zuerst der Begriff "Kubernetes Distribution" geklärt werden. Wird Kubernetes direkt aus dem Open Source Kubernetes Projekt installiert, erhält man "nur" die Kernkomponenten (API Server, Controller Manager, Scheduler, Kubelet, kube-proxy). Damit Kubernetes aber auch wirklich nutzbar wird,  werden viele weitere Komponenten wie etcd, Ingress Controller, Logging Server, Metrics Collector (z.B. Prometheus), Software Defined Network (SDN) usw. benötigt. Dies ist gut vergleichbar mit Linux: Der Linux Kernel alleine bringt noch nicht viel. Es braucht eine ganze Linux Distribution, die eine Shell, das Paketmanagement, den Bootprozess und vieles mehr zur Verfügung stellt.</p>
            <p><em>OpenShift ist eine Kubernetes Distribution und macht aus Kubernetes ein Produkt.</em></p>
            <p>Eine "Minimum Viable Kubernetes Distribution" benötigt folgende zusätzliche Komponenten und Tools für einen produktiven Betrieb:</p>
            <div style="padding-left: 30px;">▸<b> Installations- und Upgrademechanismus:</b> Für eine automatisierte Installation aller involvierten Komponenten.</div>
            <div style="padding-left: 30px;">▸<b> SDN (Software Defined Network):</b> Pods müssen untereinander kommunizieren können, egal wo sie laufen. Dies stellt das SDN sicher.</div>
            <div style="padding-left: 30px;">▸<b> Ingress Controller:</b> Damit der Benutzerzugriff auf die auf dem Cluster laufende Applikationen möglich ist.</div>
            <div style="padding-left: 30px;">▸<b> Authentication:</b> Eine zentrale Benutzer- und Gruppendatenbank stellt den authentisierten und autorisierten Zugriff zur Verfügung.</div>
            <div style="padding-left: 30px;">▸<b> Security:</b> Kubernetes führt Container via Docker oder CRI-O aus. Die Sicherheit auf dem Hostsystem muss entsprechend gewährleistet sein.</div>
            <div style="padding-left: 30px;">▸<b> Persistent Storage:</b> Stateful Applikationen wie Datenbanken benötigen persistenten Storage.</div>
            <div style="padding-left: 30px;">▸<b> Monitoring:</b> Ständige Überwachung aller Clusterkomponenten und Applikationen.</div>
            <div style="padding-left: 30px;">▸<b> Backup:</b> Sicherung der Clusterkomponenten und persistenten Daten.</div>
            <p>Optional werden weitere Komponente empfohlen:</p>
            <div style="padding-left: 30px;">▸ Zentrales Logging mit grafischer Aufbereitung und Suchfunktion</div>
            <div style="padding-left: 30px;">▸ Applikations- und Cluster Metrics inkl. Alerting</div>
            <h4>4. OpenShift als Kubernetes Distribution</h4>
            <p>Im Kern basiert OpenShift zu 100% auf Kubernetes, bringt aber als Kubernetes Distribution alles mit, was zur Benutzung eines Kubernetes Clusters benötigt wird. Um nur die wichtigsten Funktionen zu nennen:</p>
            <div style="padding-left: 30px;">▸<b> Operations Tools:</b> Ein offizieller Weg via Ansible ermöglicht es, den gesamten Lifecycle von OpenShift durchzuführen. Dazu gehört die automatisierte Installation, wie auch Upgrades auf neuere Versionen von OpenShift. Mit OpenShift 4 beginnt eine neue Ära mit einem neuen Installations- und Operationsprozess, basierend auf Kubernetes Operators.</div>
            <div style="padding-left: 30px;">▸<b> Router:</b> Der OpenShift Router (Ingress Controller) - basierend auf HAProxy - sorgt dafür, dass der Zugriff auf Applikationen innerhalb des Clusters über HTTP(S) ermöglicht wird.</div>
            <div style="padding-left: 30px;">▸<b> Multi-Tenancy:</b> Die im Kern eingebaute Multi-Tenancy über OpenShift Projekte, RBAC und weiteren Konzepten ermöglicht die Benutzung der Plattform durch verschiedene Tenants (Kunden).</div>
            <div style="padding-left: 30px;">▸<b> Authentication:</b> Es werden die unterschiedlichsten Authentication Backends unterstützt, allen voran LDAP, ActiveDirectory und viele mehr.</div>
            <div style="padding-left: 30px;">▸<b> Metrics:</b> Die mitgelieferte Metrics Komponente sammelt alle verfügbaren Messwerte (RAM, CPU, Netzwerk) der auf dem Cluster laufenden Applikationen und visualisiert diese in der Webkonsole.</div>
            <div style="padding-left: 30px;">▸<b> Central Logging:</b> Alle von der Applikation auf stdout geloggten Zeilen werden automatisch von der zentralen Logging Komponente gesammelt und über die Webkonsole dem Benutzer zur Verfügung gestellt.</div>
            <div style="padding-left: 30px;">▸<b> Security:</b> Die Plattform ist auf höchste Sicherheit ausgelegt. So sorgen z.B. Sicherheitsmassnahmen im Kernel von Red Hat Enterprise Linux wie SELinux dafür, dass die Sicherheit der Container gewährleistet ist. Weitere Massnahmen wie "Security Context Constraints" (SCC) und das Verhindern von Root Containern sorgen für weitere Sicherheit.</div>
            <div style="padding-left: 30px;">▸<b> Builds und Pipelines:</b> Direkt im Cluster integrierte Build- und Pipeline-Funktionalitäten ermöglichen einen komplett integrierten CI/CD Workflow.</div>
            <div style="padding-left: 30px;">▸<b> Webkonsole:</b> Alle Vorgänge auf dem Cluster werden für den Anwender der Plattform in einer Webkonsole graphisch dargestellt und ermöglichen einen einfachen und schnellen Einstieg in die Benutzung von Kubernetes.</div>
            <div style="padding-left: 30px;">▸<b> SDN:</b> Das mitgelieferte Software Defined Networking sorgt für die Konnektivität zwischen den auf der Plattform laufenden Pods und für eine angemessene Netzwerksicherheit mit Network Policies.</div>
            <div style="padding-left: 30px;">▸<b> Container Registry:</b> Docker / Container Images werden in der mitgelieferten Registry gespeichert und zum Deployment auf die Worker Nodes benutzt.</div>
            <p>Alle diese von Haus aus mitgelieferten Funktionalitäten lassen sich zu jedem Kubernetes Cluster hinzufügen, was jedoch mit einem hohen Aufwand verbunden ist. Dies ist vergleichbar mit dem Bau einer eigenen Linux Distribution, wie z.B. das <a href="http://www.linuxfromscratch.org/">"Linux From Scratch"</a> veranschaulicht. Für Kubernetes existiert eine ähnliche Anleitung, genannt <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">"Kubernetes The Hard Way"</a>.</p>
            <h4>5. OpenShift als PaaS</h4>
            <p>Die Stärke von Kubernetes liegt in der Container Orchestrierung. Zusätzlich dazu bietet OpenShift klassische Platform-as-a-Service (PaaS) Funktionen. Eine davon ist das automatische Builden und Deployen von Applikationscode direkt ab einem Git Repository. Trotzdem hat man als Anwender der Plattform dank der grossen Flexibilität immer die Wahl, ob man die integrierten Buildfunktionen nutzen oder doch lieber ausserhalb des Clusters builden möchte. Dies lässt sich für jedes Deployment entscheiden. So können auf einem Cluster beide Arten verwendet werden.</p>
            <h4>6. OpenShift als Upstream zu Kubernetes</h4>
            <p>Viele Entwicklungen in Kubernetes stammen ursprünglich aus OpenShift. Als bestes Beispiel lässt sich RBAC (Role Based Access Control) nennen. Dieses Feature ist seit der ersten OpenShift-Version Bestandteil und wurde sukzessive in Kubernetes eingebaut. RBAC ist seit Kubernetes Version 1.6 fester Bestandteil von Kubernetes. Auch das OpenShift "Route"- oder das "DeploymentConfiguration"-Objekt hat die heutigen Objekte "Ingress" bzw. "Deployment" in Kubernetes massgeblich mitgeprägt.</p>
            <p>Da OpenShift zu 100% auf Kubernetes basiert, werden auch alle Kubernetes Native Workloads unterstützt, wie z.B. das "Deployment"- oder das "Ingress"-Objekt.</p>
            <p>Schaut man etwas genauer auf die <a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1">Contributor-Statistiken</a>, dann stellt man fest, dass Red Hat die Nummer 2 der Contributor-Firmen ist (nur Google ist noch weiter vorn). Somit ist Red Hat massgeblich an der Entwicklung von Kubernetes beteiligt. Mit dem Kauf der Firma CoreOS hat sich Red Hat geballtes Kubernetes Know-how angeeignet. Das Ergebnis der CoreOS Integration in Red Hat ist die Verschmelzung von OpenShift und Tectonic zu OpenShift Version 4.</p>
            <h4>7. Alternativen zu OpenShift</h4>
            <p>OpenShift ist nicht die einzige Kubernetes Distribution auf dem Markt. Ein kurzer Vergleich zeigt die Unterschiede:</p>
            <div style="padding-left: 30px;">▸ <b> Cloud Vendor Kubernetes:</b>Die grossen Clouds bieten ihre eigenen Kubernetes Distributionen als Service an. Diese sind auf die jeweiligen Clouds zugeschnitten und werden von den Anbietern gepflegt. Eine Installation auf der eigenen Private Cloud oder auf anderen Public Clouds ist nicht möglich.</div>
            <div style="padding-left: 50px;">▸ <a href="https://aws.amazon.com/de/eks/">Amazon Elastic Container Service for Kubernetes (Amazon EKS)</a></div>
            <div style="padding-left: 50px;">▸ <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a></div>
            <div style="padding-left: 50px;">▸ <a href="https://azure.microsoft.com/de-de/services/kubernetes-service/">Azure Kubernetes Service (AKS)</a></div>
            <div style="padding-left: 50px;">▸ <a href="https://www.ibm.com/cloud/container-service">IBM Cloud Kubernetes Service</a></div>
            <div style="padding-left: 50px;">▸ <a href="https://www.alibabacloud.com/de/product/kubernetes"> Alibaba Cloud Container Service for Kubernetes</a></div>
            <div style="padding-left: 30px;">▸ <b><a href="https://rancher.com/kubernetes/">Rancher:</a></b>Seit der Version 2.0 fokussiert sich Rancher zu 100% auf Kubernetes und bietet als grosse Stärke eine Multi-Cluster Verwaltungsfunktion. So können mit Rancher Kubernetes Cluster in der Cloud (z.B. auf Amazon oder Google) zentral verwaltet werden, wie auch Kubernetes Cluster mit der "Rancher Kubernetes Engine" auf eigene VMs. Mit dem Webinterface gestaltet sich das Aufsetzen eines neuen Clusters sehr einfach und Applikationsdeployments mittels Helm sind auch direkt verfügbar.</div>
            <div style="padding-left: 30px;">▸ <b><a href="https://ubuntu.com/kubernetes">Canonical / Ubuntu Kubernetes</a></b> (Charmed Kubernetes): Plattform basierend auf Ubuntu, welches Juju als Installationstool verwendet. In Partnerschaft mit Google und Rancher wird in Zukunft eine Hybrid-Cloud-Lösung angeboten.</div>
            <div style="padding-left: 30px;">▸ <b><a href="https://www.suse.com/de-de/products/caas-platform/">SUSE CaaS-Plattform:</a></b> Eine neue Plattform, basierend auf SUSE MicroOS. Mittels Salt wird die Konfigurationsverwaltung sichergestellt. Unter folgendem Link kann am Beta Programm teilgenommen werden:<a href="https://www.suse.com/betaprogram/caasp-beta/"> SUSE CaaS Platform Beta.</a></div>
            <p>Weitere Kubernetes Distributionen aufgelistet:</p>
            <div style="padding-left: 30px;">▸ <b><a href="https://www.kontena.io/pharos/">Kontena Pharos</a></b></div>
            <div style="padding-left: 30px;">▸ <b><a href="https://www.cisco.com/c/en/us/products/cloud-systems-management/container-platform/index.html/">Cisco Container Platform</a></b></div>
            <div style="padding-left: 30px;">▸ <b><a href="https://cloud.netapp.com/kubernetes-service">NetApp Kubernetes Service</a></b></div>
            <div style="padding-left: 30px;">▸ Und noch mehr aus der Kubernetes Dokumentation: <a href="https://kubernetes.io/docs/setup/">Getting started</a></div>
            <h4>8. Vendor Lock-ins</h4>
            <p>Ein sehr wichtiger zu beachtender Aspekt ist der Cloud- und/oder Vendor-Lock-In. Viele der Kubernetes Distributionen haben ihre eigene Eigenschaften, die unter Umständen nicht miteinander kompatibel sind. Am Beispiel der "Cloud-Vendor"-Distributionen: Diese können nur in der entsprechenden Cloud benutzt werden. Möchte man jedoch einen Hybrid-Cloud-Ansatz verfolgen, ist dies durch den Lock-In nicht möglich. Im Gegenzug ermöglicht eine selbst installierbare Distribution wie OpenShift diese Option.</p>
            <p>Reine Open Source Distributionen ohne Herstellersupport sind für produktive Umgebungen nicht zu empfehlen.</p>
            <h4>9. APPUiO - Swiss Container Platform</h4>
            <p>Dem aufmerksamen Leser ist bestimmt aufgefallen, dass zwischen der "Minimum Viable Kubernetes Distribution" und OpenShift gewisse Diskrepanzen bestehen. Genau dort setzt APPUiO an: Wir veredeln OpenShift zu einer vollständigen, production-ready Kubernetes Distribution, indem wir Managed Services anbieten. Wir überwachen und sichern den Clusterstatus automatisch, kümmern uns um regelmässige Updates, beheben Fehler, stellen Persistent Storage zur Verfügung und helfen mit unserem Know-how das Beste aus der Plattform herauszuholen. Durch unsere Erfahrung im Setup und Betrieb von OpenShift Clustern rund um die Welt, bieten wir Managed OpenShift Cluster auf nahezu jeder Public, Private oder On-Premise Cloud an. APPUiO hilft gerne bei der Evaluation, Integration und Betrieb und unterstützt mit ihrer langjährigen Kubernetes und OpenShift Erfahrung.</p>
            <h4>10. Wo kann ich mehr erfahren?</h4>
            <p>Am <a href="https://www.meetup.com/de-DE/Cloud-Native-Computing-Switzerland/events/251026175/"> Cloud Native Meetup vom 28. August 2018</a> haben wir übers Thema Kubernetes Distribution berichtet. Die Slides dazu sind auf <a href="https://speakerdeck.com/tobru/the-anatomy-of-a-kubernetes-distribution">Speaker Deck</a> zu finden. Ein empfehlenswerter Blogpost zum Thema Kubernetes Distributionen (auf Englisch) findet ihr hier: <a href="https://cloudowski.com/articles/10-differences-between-openshift-and-kubernetes/">10 most important differences between OpenShift and Kubernetes</a>.</p>
            <p>Wenn du mehr über OpenShift erfahren willst, besuche uns an unserem Stand am Red Hat Forum Zürich am 10. September 2019 - 8:00 bis 18:00 im StageOne Zürich-Oerlikon.</p>
            <p>Das gesamte APPUiO-Team freut sich auf deinen Besuch!</p>        

- day: 31
  month: Jul
  title: "Neu: OpenShift Dev Schulung"
  subtitle: |
            <img src="/images/blogposts/Fotoxy.png" alt="DevSchulung"/>
            Ab sofort bietet APPUiO OpenShift Dev Schulungen an. Wir zeigen dir, wie du deine Business Applikationen auf der OpenShift Plattform entwickeln, betreiben und monitoren kannst.
  details: |
            <p>Um unser langjähriges Wissen im Bereich der Software Entwicklung, Automatisierung und Betrieb von Cloud Native Applikationen auf OpenShift Container Plattformen weiterzugeben, haben wir die zweitägige OpenShift Dev Schulung erarbeitet. Sie baut das Know-How der Teilnehmer in einer guten Mischung aus Theorie und Hands-on Lab auf.</p>
            <p>Die Schulung beinhaltet folgende Themen, kann aber je nach Bedürfnissen angepasst werden:</p>
            <div><b>Agenda – day 1</b></div>
            <div>1. Introduction Linux Container / OpenShift Plattform</div>
            <div>2. Application Architecture / Cloud Native / 12 Factor Apps / Service Broker</div>
            <div>3. Introduction to develop an Application on OpenShift</div>
            <div>4. Hands-on: Build Strategies on OpenShift (s2i, binary, docker)</div>
            <div>5. Application Configuration / Service Discovery</div>
            <div>6. Hands-on: Build applications with different runtimes & deploy them on Openshift</div>
            <div>7. Share & Discuss exercise results / Wrapup</div>
            <br>
            <div><b>Agenda – day 2</b></div>
            <div>1. Recap day 1</div>            
            <div>2. Troubleshooting & Debugging</div>            
            <div>3. Hands-on - Troubleshooting & Debugging</div> 
            <div>4. Backup / restore for Applications</div>
            <div>5. CI/CD Pipelines Introduction and Concepts</div>
            <div>6. Q&A for compamy specific usecases</div>
            <br>
            <div><b>Zusätzliche Themen auf Anfrage</b></div>
            <div>1. Coolstore Example (Microservice application showing different concepts and languages)</div>            
            <div>2. Operator Framework</div>            
            <div>3. Application Monitoring with Prometheus</div> 
            <div style="padding-left: 30px;">▸ Deploy Prometheus and Grafana</div>
            <div style="padding-left: 30px;">▸ Monitoring of a Spring Boot application</div>
            <p>Die Schulung wird von Christoph Raaflaub, unserem Software Architect & Middleware Engineer, durchgeführt. Christoph ist unser Spezialist in Sache automatisierte Software Delivery und teilt gerne seine langjährige Erfahrung mit euch.</p>
            <p>Die OpenShift Dev Schulung wird bis max. 8 Teilnehmer durchgeführt. Auf Anfrage ist eine Durchführung auch für ganze Gruppen möglich, dediziert oder bei dir am Standort. Die Schulung ist in Deutsch oder Englisch möglich.</p>
            <p>Ist das etwas für dich und deine Mitarbeiter? Gerne senden wir dir das Angebot zur OpenShift Dev Schulung zu und freuen uns auf deine Kontaktaufnahme.</p>
            <div><b>Ort:</b> Dediziert bei dir (oder an unseren Standorten in Bern/Zürich)</div>
            <div><b>Kosten:</b> auf Anfrage</div>
            <div><b>Kontakt für Rückfragen:</b> <a href="mailto:hello@appuio.ch">hello@appuio.ch</a>, Anna Pfeifhofer</div>

- day: 23
  month: Mai
  title: "Was Arcades, gelbe Socken und Container gemeinsam haben"
  subtitle: |
            <img src="/images/blogposts/DevOps.jpg" alt="DevOps"/>
            Am 14. und 15. Mai 2019 fanden die DevOpsDays 2019 in Spreitenbach statt. APPUiO war als Platin-Sponsor und einem eigenen Stand vertreten. Aber was hat das alles nun mit Arcades, gelben Socken und Container zu tun? Erfahrt mehr in diesem Blogpost.
  details: |
            <p><em>Montag, 13. Mai, mittags</em><br />Der Bus («Danke YOKKO») ist bereit, die Reise nach Spreitenbach in die Umweltarena anzutreten.  Das Ziel: Die DevOpsDays 2019. Bepackt mit lauter gelben Sachen trifft sich das APPUiO-Team in Zürich. Nun heisst es: anpacken, aufbauen und schlussendlich...sich freuen.</p>
            <img src="/images/blogposts/DevOps1.JPG" alt="DevOps1"/>
            <p><em>Montag, 13. Mai, abends</em><br />«Yes it's done» - Unser APPUiO-Stand steht bereit für die nächsten zwei Tage. Alle halfen mit viel Fleiss und Liebe zum Detail den Stand zu schmücken und für die Teilnehmenden interessant zu gestalten. Das buntgemischte APPUiO-Team aus <a href="https://vshn.ch/">VSHNers</a> und <a href="https://www.puzzle.ch/de/">Puzzlers</a> gibt dem Stand wahrscheinlich das gewisse Etwas. Das Ergebnis lässt sich jedenfalls zeigen.</p>
            <img src="/images/blogposts/DevOps2.JPG" alt="DevOps2"/>
            <p><em>Dienstag, 14. Mai, morgens</em><br />Das Team wartet gespannt auf die Gäste und verleiht dem Stand den letzten Schliff, damit die Teilnehmenden ein optimales APPUiO-Erlebnis geniessen können. Zwei Arcades stehen für die anstehenden Battles bereit. Im Gewinner-Pot liegen Vouchers von APPUiO und <a href="https://www.cloudscale.ch/de/">cloudscale.ch</a> und eine Lego Saturn V Rakete. Weiteres Spielvergnügen bietet der Retro-Rohrfernseher mit der angeschlossenen Nintendo 64. Auf der gemütlichen Sofaecke lässt es sich verweilen, sich austauschen oder die Präsentationen verfolgen.</p>
            <p>Bereits vor den ersten Präsentationen wagen sich einige Teilnehmenden in die Höhle der Arcade-Löwen und wollen den Highscore erreichen. Begeisterung zeigen die Besucher auch für unser <a href="https://appuio.ch/blog.html#APPUiO%20an%20den%20DevOpsDays%202018">APPUiOli</a>. In Gesprächen erfahren sie zudem mehr über APPUiO, die nächsten Etappen und Ziele.</p>
            <img src="/images/blogposts/DevOps3.JPG" alt="DevOps3"/>
            <img src="/images/blogposts/DevOps4.JPG" alt="DevOps4"/>
            <img src="/images/blogposts/DevOps5.JPG" alt="DevOps5"/>
            <p><em>Dienstag, 14. Mai, abends</em><br />Der erste Tag ist geschafft! Bei einem Bier, einem super Apéro und tollen Gesprächen lässt sich der Tag wunderbar ausklingen. Das Team durfte viele neue Kontakte knüpfen und freut sich über das grosse Interesse an APPUiO. Auch unsere Community war stolz vertreten, viele der Community-Mitglieder besuchte APPUiO am Stand. Dabei merken wir, wie gross die Community von APPUiO bereits geworden ist. Dies lässt auf weitere gemeinsame Innovationen und coole Events hoffen. Wir freuen uns, auf das, was kommt!</p>
            <img src="/images/blogposts/DevOps6.jpg" alt="DevOps6"/>
            <p><em>Mittwoch, 15. Mai, mittags</em><br />Neuer Tag, neues Glück? Das denken sich wahrscheinlich auch einige der Besucher und wagen erneut ein Arcade-Battle. Für die APPUiO-Sockenträger unter den Besuchern gibt es eine kleine Überraschung: Als Merci für ihre Treue erhalten sie ein Mandelbärchen. ...und wer nicht Süsses mag, kann auch Saures haben… Die Gummibärchen im APPUiO-Style helfen nicht nur bei den Spielbattles als Nervennahrung, sondern auch unserem <a href="https://vshn.ch/vshn/">Markus Speth</a>. Der APPUiO-Marketingexperte darf am Nachmittag in einem Pitch APPUiO vorstellen.</p>
            <img src="/images/blogposts/DevOps7.jpg" alt="DevOps7"/>
            <p><em>Mittwoch, 15. Mai, abends</em><br />Auch dieser Tag geht langsam zu Ende. Als Krönung werden die drei Gewinner des Arcade-Wettbewerbs - Alvise Dorigo, Michael Gerber und Carlo Speranza - geehrt. Müde aber zufrieden verabschiedet das APPUiO-Team die letzten Besucher. Nach der kurzen aber intensiven Aufräumaktion finden dann auch die «APPUiOler» den Weg nach Hause. Wir bedanken uns ganz herzlich für die tollen Bekanntschaften, spannenden Gesprächen und für euren Besuch an unserem Stand. Es hat uns einen Riesenspass gemacht! Und wir freuen uns, wenn es wieder heisst: DevOpsDays 2020!</p>
            <img src="/images/blogposts/DevOps8.JPG" alt="DevOps8"/>
            <img src="/images/blogposts/DevOps9.JPG" alt="DevOps9"/>
            <p>Und falls du Nervennahrung brauchst, kalte Füsse hast oder deinen Laptop in APPUiO-Style aufpimpen willst, dann meld dich bei uns.  #APPUiOGummibärchen #APPUiOSocken #Stickers ;-)</p>
            <p>Bis dann!</p>
            <img src="/images/blogposts/DevOps10.jpg" alt="DevOps10"/>

- day: 11
  month: Apr
  title: "Ein kurzer Rückblick auf das erste beerup von APPUiO"
  subtitle: |
            <img src="/images/blogposts/beerup.jpg" alt="Beerup"/>
            Am 28. März fand das erste beerup von APPUiO in der Rüsterei in Zürich statt. Dabei durften wir von den Inputs und Ideen unserer APPUiO Community profitieren und das Beisammensein bei einem Bier geniessen.
  details: |
            <p>Der Begriff beerup ist mit dem Ziel entstanden, die Community von APPUiO in ungezwungener Atmosphäre zu vereinen und dabei über die Bedürfnisse der Community zum Thema Container und OpenShift zu sprechen. Was könnte APPUiO der Community bieten? Wie können wir noch mehr voneinander profitieren? Denn ihr wisst: Zusammen sind wir stärker, können Grosses in der IT-Welt erreichen und gemeinsam macht es einfach mehr Spass :-)</p>
            <p>Das erste beerup hat am 28. März im Anschluss an dem OpenShift RoundTable von Red Hat stattgefunden. Dabei durften wir rund 75 Gäste in der Rüsterei in Zürich empfangen. Unsere Community konnte sich bei einem Apéro verpflegen und sich über dies und jenes austauschen. In den vielen tollen Gesprächen stiess das APPUiO-Team auf verschiedenste Ideen und Bedürfnisse der Community. Durch das Feedback jedes Einzelnen durften wir eine grosse Anzahl an Inputs aufnehmen. Dafür bedanken wir uns noch einmal herzlich bei euch und bei Red Hat für die Unterstützung.</p>
            <img src="/images/blogposts/RoundTable.JPG" alt="Roundtable"/>
            <img src="/images/blogposts/beerup1.jpg" alt="Beerup1"/>
            <br>
            <b>Doch wie geht es nun weiter?</b> 
            <div>Das APPUiO-Team wird sich einen Überblick über eure Ideen verschaffen und versuchen, sie baldmöglichst zu realisieren. Du kannst dich also jetzt schon über die weitere Entwicklung von APPUiO freuen!</div>
            <b>Du hast noch einen Input oder eine Idee aber konntest diese nicht mitteilen?</b> 
            <div>Wir freuen uns jederzeit über euer Feedback. <a href="mailto:hello@appuio.ch">Schreib uns</a> einfach und teile uns mit, wie wir APPUiO weiter verbessern können.</div>
            <p>Es hat Spass gemacht, eine so grosse Anzahl von Leuten aus den unterschiedlichsten Branchen und Kantonen bei uns am beerup begrüssen zu dürfen, mit euch zu plaudern und euch näher kennenzulernen.</p>
            <p>Wir wünschen euch eine gute Zeit und bis zum nächsten Mal!</p>
